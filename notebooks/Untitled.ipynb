{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damage Propagation Modeling.pdf test_FD002.txt\n",
      "RUL_FD001.txt                   test_FD003.txt\n",
      "RUL_FD002.txt                   test_FD004.txt\n",
      "RUL_FD003.txt                   train_FD001.txt\n",
      "RUL_FD004.txt                   train_FD002.txt\n",
      "readme.txt                      train_FD003.txt\n",
      "test_FD001.txt                  train_FD004.txt\n"
     ]
    }
   ],
   "source": [
    "!ls $raw_data_cache/CMAPSSData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set: FD001\n",
      "Train trjectories: 100\n",
      "Test trajectories: 100\n",
      "Conditions: ONE (Sea Level)\n",
      "Fault Modes: ONE (HPC Degradation)\n",
      "\n",
      "Data Set: FD002\n",
      "Train trjectories: 260\n",
      "Test trajectories: 259\n",
      "Conditions: SIX \n",
      "Fault Modes: ONE (HPC Degradation)\n",
      "\n",
      "Data Set: FD003\n",
      "Train trjectories: 100\n",
      "Test trajectories: 100\n",
      "Conditions: ONE (Sea Level)\n",
      "Fault Modes: TWO (HPC Degradation, Fan Degradation)\n",
      "\n",
      "Data Set: FD004\n",
      "Train trjectories: 248\n",
      "Test trajectories: 249\n",
      "Conditions: SIX \n",
      "Fault Modes: TWO (HPC Degradation, Fan Degradation)\n",
      "\n",
      "\n",
      "\n",
      "Experimental Scenario\n",
      "\n",
      "Data sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine � i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n",
      "\n",
      "The engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective of the competition is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate. Also provided a vector of true Remaining Useful Life (RUL) values for the test data.\n",
      "\n",
      "The data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n",
      "1)\tunit number\n",
      "2)\ttime, in cycles\n",
      "3)\toperational setting 1\n",
      "4)\toperational setting 2\n",
      "5)\toperational setting 3\n",
      "6)\tsensor measurement  1\n",
      "7)\tsensor measurement  2\n",
      "...\n",
      "26)\tsensor measurement  26\n",
      "\n",
      "\n",
      "Reference: A. Saxena, K. Goebel, D. Simon, and N. Eklund, �Damage Propagation Modeling for Aircraft Engine Run-to-Failure Simulation�, in the Proceedings of the Ist International Conference on Prognostics and Health Management (PHM08), Denver CO, Oct 2008.\n"
     ]
    }
   ],
   "source": [
    "!cat $raw_data_cache/CMAPSSData/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damage Propagation Modeling.pdf\n",
      "RUL_FD001.txt\n",
      "RUL_FD002.txt\n",
      "RUL_FD003.txt\n",
      "RUL_FD004.txt\n",
      "readme.txt\n",
      "test_FD001.txt\n",
      "test_FD002.txt\n",
      "test_FD003.txt\n",
      "test_FD004.txt\n",
      "train_FD001.txt\n",
      "train_FD002.txt\n",
      "train_FD003.txt\n",
      "train_FD004.txt\n"
     ]
    }
   ],
   "source": [
    "parent_dir = raw_data_cache / \"CMAPSSData\"\n",
    "\n",
    "train_df = []\n",
    "test_df = []\n",
    "labels = []\n",
    "\n",
    "for fn in sorted(list(parent_dir.iterdir())):\n",
    "    print(fn.name)\n",
    "    if \"train\" in fn.name:\n",
    "        df = pd.read_table(fn, sep=\" \", header=None)\n",
    "        train_df.append(df)\n",
    "\n",
    "    if \"test\" in fn.name:\n",
    "        df = pd.read_table(fn, sep=\" \", header=None)\n",
    "        test_df.append(df)\n",
    "\n",
    "    if \"RUL\" in fn.name:\n",
    "        label = pd.read_table(fn, sep=\" \", header=None)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "360\n",
      "460\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\"unit_no\", \"t_cycles\", \"setting_1\", \"setting_2\", \"setting_3\"] + [\n",
    "    f\"sensor_{i}\" for i in range(21)\n",
    "]\n",
    "\n",
    "shape_offset = 0\n",
    "for i in range(len(train_df)):\n",
    "    df = train_df[i].copy(deep=True)\n",
    "    df.drop([26, 27], axis=1, inplace=True)\n",
    "    df.columns = feature_names\n",
    "    \n",
    "    max_life = df.groupby('unit_no')['t_cycles'].max()\n",
    "    max_life.rename('max_life', inplace=True)\n",
    "    df = df.join(max_life, how='left', on='unit_no')\n",
    "    df = df.assign(rul = df.max_life - df.t_cycles)\n",
    "    \n",
    "    # reassign unit numbers\n",
    "    df = df.assign(unit_no = df.unit_no+shape_offset)\n",
    "    print(shape_offset)\n",
    "    shape_offset += df.unit_no.nunique()\n",
    "    train_df[i] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(train_df, ignore_index=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [x for x in feature_names if x not in ['unit_no','max_life','rul']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t_cycles',\n",
       " 'setting_1',\n",
       " 'setting_2',\n",
       " 'setting_3',\n",
       " 'sensor_0',\n",
       " 'sensor_1',\n",
       " 'sensor_2',\n",
       " 'sensor_3',\n",
       " 'sensor_4',\n",
       " 'sensor_5',\n",
       " 'sensor_6',\n",
       " 'sensor_7',\n",
       " 'sensor_8',\n",
       " 'sensor_9',\n",
       " 'sensor_10',\n",
       " 'sensor_11',\n",
       " 'sensor_12',\n",
       " 'sensor_13',\n",
       " 'sensor_14',\n",
       " 'sensor_15',\n",
       " 'sensor_16',\n",
       " 'sensor_17',\n",
       " 'sensor_18',\n",
       " 'sensor_19',\n",
       " 'sensor_20']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model0(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.i_h = nn.Embedding(nv,nh)  # green arrow\n",
    "#         self.h_h = nn.Linear(nh,nh)     # brown arrow\n",
    "#         self.h_o = nn.Linear(nh,nv)     # blue arrow\n",
    "#         self.bn = nn.BatchNorm1d(nh)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         h = self.bn(F.relu(self.h_h(self.i_h(x[:,0]))))\n",
    "#         if x.shape[1]>1:\n",
    "#             h = h + self.i_h(x[:,1])\n",
    "#             h = self.bn(F.relu(self.h_h(h)))\n",
    "#         if x.shape[1]>2:\n",
    "#             h = h + self.i_h(x[:,2])\n",
    "#             h = self.bn(F.relu(self.h_h(h)))\n",
    "#         return self.h_o(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model0(nn.Module):\n",
    "    def __init__():\n",
    "        super(self, nn.Module).__init__()\n",
    "        self.linear1 = nn.Linear(25,25)\n",
    "        self.linear_beta = nn.Linear(25,1)\n",
    "        self.linear_alpha = nn.Linear(25,1)\n",
    "        self.bn = nn.BatchNorm1d(25)\n",
    "        self.softplus = nn.Softplus(1)\n",
    "    def forward(x):\n",
    "        h = self.bn(F.relu(self.linear1(x[:,0])))\n",
    "        if x.shape[1]>1:\n",
    "            h = h + x[:,1]\n",
    "            h = self.bn(F.relu(self.linear1(x[:,1])))\n",
    "        if x.shape[1]>2:\n",
    "            h = h + x[:,2]\n",
    "            h = self.bn(F.relu(self.linear1(x[:,2])))\n",
    "        return self.softplus(self.linear_beta(h)), torch.exp(self.linear_alpha(h))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_loglikelihood_continuous(a_, b_, y_, u_,name=None):\n",
    "    ya = tf.div(y_+1e-35,a_)\n",
    "    return(\n",
    "        tf.mul(u_,\n",
    "               tf.log(b_)+tf.mul(b_,tf.log(ya))\n",
    "              )- \n",
    "        tf.pow(ya,b_)\n",
    "    )\n",
    "\n",
    "def weibull_loglikelihood_discrete(a_, b_, y_, u_, name=None):\n",
    "    with tf.name_scope(name):\n",
    "        hazard0 = tf.pow(tf.div(y_+1e-35,a_),b_) \n",
    "        hazard1 = tf.pow(tf.div(y_+1,a_),b_)\n",
    "    return(tf.mul(u_,tf.log(tf.exp(hazard1-hazard0)-1.0))-hazard1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learning-pytorch]",
   "language": "python",
   "name": "conda-env-learning-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
